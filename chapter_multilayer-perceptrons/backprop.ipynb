{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40d6b8e7",
      "metadata": {
        "origin_pos": 0,
        "id": "40d6b8e7"
      },
      "source": [
        "# Forward Propagation, Backward Propagation, and Computational Graphs\n",
        ":label:`sec_backprop`\n",
        "\n",
        "So far, we have trained our models\n",
        "with minibatch stochastic gradient descent.\n",
        "However, when we implemented the algorithm,\n",
        "we only worried about the calculations involved\n",
        "in *forward propagation* through the model.\n",
        "When it came time to calculate the gradients,\n",
        "we just invoked the backpropagation function provided by the deep learning framework.\n",
        "\n",
        "The automatic calculation of gradients\n",
        "profoundly simplifies\n",
        "the implementation of deep learning algorithms.\n",
        "Before automatic differentiation,\n",
        "even small changes to complicated models required\n",
        "recalculating complicated derivatives by hand.\n",
        "Surprisingly often, academic papers had to allocate\n",
        "numerous pages to deriving update rules.\n",
        "While we must continue to rely on automatic differentiation\n",
        "so we can focus on the interesting parts,\n",
        "you ought to know how these gradients\n",
        "are calculated under the hood\n",
        "if you want to go beyond a shallow\n",
        "understanding of deep learning.\n",
        "\n",
        "In this section, we take a deep dive\n",
        "into the details of *backward propagation*\n",
        "(more commonly called *backpropagation*).\n",
        "To convey some insight for both the\n",
        "techniques and their implementations,\n",
        "we rely on some basic mathematics and computational graphs.\n",
        "To start, we focus our exposition on\n",
        "a one-hidden-layer MLP\n",
        "with weight decay ($\\ell_2$ regularization, to be described in subsequent chapters).\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "*Forward propagation* (or *forward pass*) refers to the calculation and storage\n",
        "of intermediate variables (including outputs)\n",
        "for a neural network in order\n",
        "from the input layer to the output layer.\n",
        "We now work step-by-step through the mechanics\n",
        "of a neural network with one hidden layer.\n",
        "This may seem tedious but in the eternal words\n",
        "of funk virtuoso James Brown,\n",
        "you must \"pay the cost to be the boss\".\n",
        "\n",
        "\n",
        "For the sake of simplicity, let's assume\n",
        "that the input example is $\\mathbf{x}\\in \\mathbb{R}^d$\n",
        "and that our hidden layer does not include a bias term.\n",
        "Here the intermediate variable is:\n",
        "\n",
        "$$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x},$$\n",
        "\n",
        "where $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\n",
        "is the weight parameter of the hidden layer.\n",
        "After running the intermediate variable\n",
        "$\\mathbf{z}\\in \\mathbb{R}^h$ through the\n",
        "activation function $\\phi$\n",
        "we obtain our hidden activation vector of length $h$:\n",
        "\n",
        "$$\\mathbf{h}= \\phi (\\mathbf{z}).$$\n",
        "\n",
        "The hidden layer output $\\mathbf{h}$\n",
        "is also an intermediate variable.\n",
        "Assuming that the parameters of the output layer\n",
        "possess only a weight of\n",
        "$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$,\n",
        "we can obtain an output layer variable\n",
        "with a vector of length $q$:\n",
        "\n",
        "$$\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}.$$\n",
        "\n",
        "Assuming that the loss function is $l$\n",
        "and the example label is $y$,\n",
        "we can then calculate the loss term\n",
        "for a single data example,\n",
        "\n",
        "$$L = l(\\mathbf{o}, y).$$\n",
        "\n",
        "As we will see the definition of $\\ell_2$ regularization\n",
        "to be introduced later,\n",
        "given the hyperparameter $\\lambda$,\n",
        "the regularization term is\n",
        "\n",
        "$$s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_\\textrm{F}^2 + \\|\\mathbf{W}^{(2)}\\|_\\textrm{F}^2\\right),$$\n",
        ":eqlabel:`eq_forward-s`\n",
        "\n",
        "where the Frobenius norm of the matrix\n",
        "is simply the $\\ell_2$ norm applied\n",
        "after flattening the matrix into a vector.\n",
        "Finally, the model's regularized loss\n",
        "on a given data example is:\n",
        "\n",
        "$$J = L + s.$$\n",
        "\n",
        "We refer to $J$ as the *objective function*\n",
        "in the following discussion.\n",
        "\n",
        "\n",
        "## Computational Graph of Forward Propagation\n",
        "\n",
        "Plotting *computational graphs* helps us visualize\n",
        "the dependencies of operators\n",
        "and variables within the calculation.\n",
        ":numref:`fig_forward` contains the graph associated\n",
        "with the simple network described above,\n",
        "where squares denote variables and circles denote operators.\n",
        "The lower-left corner signifies the input\n",
        "and the upper-right corner is the output.\n",
        "Notice that the directions of the arrows\n",
        "(which illustrate data flow)\n",
        "are primarily rightward and upward.\n",
        "\n",
        "![Computational graph of forward propagation.](http://d2l.ai/_images/forward.svg)\n",
        ":label:`fig_forward`\n",
        "\n",
        "## Backpropagation\n",
        "\n",
        "*Backpropagation* refers to the method of calculating\n",
        "the gradient of neural network parameters.\n",
        "In short, the method traverses the network in reverse order,\n",
        "from the output to the input layer,\n",
        "according to the *chain rule* from calculus.\n",
        "The algorithm stores any intermediate variables\n",
        "(partial derivatives)\n",
        "required while calculating the gradient\n",
        "with respect to some parameters.\n",
        "Assume that we have functions\n",
        "$\\mathsf{Y}=f(\\mathsf{X})$\n",
        "and $\\mathsf{Z}=g(\\mathsf{Y})$,\n",
        "in which the input and the output\n",
        "$\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$\n",
        "are tensors of arbitrary shapes.\n",
        "By using the chain rule,\n",
        "we can compute the derivative\n",
        "of $\\mathsf{Z}$ with respect to $\\mathsf{X}$ via\n",
        "\n",
        "$$\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\textrm{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right).$$\n",
        "\n",
        "Here we use the $\\textrm{prod}$ operator\n",
        "to multiply its arguments\n",
        "after the necessary operations,\n",
        "such as transposition and swapping input positions,\n",
        "have been carried out.\n",
        "For vectors, this is straightforward:\n",
        "it is simply matrix--matrix multiplication.\n",
        "For higher dimensional tensors,\n",
        "we use the appropriate counterpart.\n",
        "The operator $\\textrm{prod}$ hides all the notational overhead.\n",
        "\n",
        "Recall that\n",
        "the parameters of the simple network with one hidden layer,\n",
        "whose computational graph is in :numref:`fig_forward`,\n",
        "are $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$.\n",
        "The objective of backpropagation is to\n",
        "calculate the gradients $\\partial J/\\partial \\mathbf{W}^{(1)}$\n",
        "and $\\partial J/\\partial \\mathbf{W}^{(2)}$.\n",
        "To accomplish this, we apply the chain rule\n",
        "and calculate, in turn, the gradient of\n",
        "each intermediate variable and parameter.\n",
        "The order of calculations are reversed\n",
        "relative to those performed in forward propagation,\n",
        "since we need to start with the outcome of the computational graph\n",
        "and work our way towards the parameters.\n",
        "The first step is to calculate the gradients\n",
        "of the objective function $J=L+s$\n",
        "with respect to the loss term $L$\n",
        "and the regularization term $s$:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial L} = 1 \\; \\textrm{and} \\; \\frac{\\partial J}{\\partial s} = 1.$$\n",
        "\n",
        "Next, we compute the gradient of the objective function\n",
        "with respect to variable of the output layer $\\mathbf{o}$\n",
        "according to the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{o}}\n",
        "= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}}\\right)\n",
        "= \\frac{\\partial L}{\\partial \\mathbf{o}}\n",
        "\\in \\mathbb{R}^q.\n",
        "$$\n",
        "\n",
        "Next, we calculate the gradients\n",
        "of the regularization term\n",
        "with respect to both parameters:\n",
        "\n",
        "$$\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}\n",
        "\\; \\textrm{and} \\;\n",
        "\\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}.$$\n",
        "\n",
        "Now we are able to calculate the gradient\n",
        "$\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$\n",
        "of the model parameters closest to the output layer.\n",
        "Using the chain rule yields:\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\textrm{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}.$$\n",
        ":eqlabel:`eq_backprop-J-h`\n",
        "\n",
        "To obtain the gradient with respect to $\\mathbf{W}^{(1)}$\n",
        "we need to continue backpropagation\n",
        "along the output layer to the hidden layer.\n",
        "The gradient with respect to the hidden layer output\n",
        "$\\partial J/\\partial \\mathbf{h} \\in \\mathbb{R}^h$ is given by\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{h}}\n",
        "= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right)\n",
        "= {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}.\n",
        "$$\n",
        "\n",
        "Since the activation function $\\phi$ applies elementwise,\n",
        "calculating the gradient $\\partial J/\\partial \\mathbf{z} \\in \\mathbb{R}^h$\n",
        "of the intermediate variable $\\mathbf{z}$\n",
        "requires that we use the elementwise multiplication operator,\n",
        "which we denote by $\\odot$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{z}}\n",
        "= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right)\n",
        "= \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'\\left(\\mathbf{z}\\right).\n",
        "$$\n",
        "\n",
        "Finally, we can obtain the gradient\n",
        "$\\partial J/\\partial \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\n",
        "of the model parameters closest to the input layer.\n",
        "According to the chain rule, we get\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
        "= \\textrm{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}\\right) + \\textrm{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n",
        "= \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Training Neural Networks\n",
        "\n",
        "When training neural networks,\n",
        "forward and backward propagation depend on each other.\n",
        "In particular, for forward propagation,\n",
        "we traverse the computational graph in the direction of dependencies\n",
        "and compute all the variables on its path.\n",
        "These are then used for backpropagation\n",
        "where the compute order on the graph is reversed.\n",
        "\n",
        "Take the aforementioned simple network as an illustrative example.\n",
        "On the one hand,\n",
        "computing the regularization term :eqref:`eq_forward-s`\n",
        "during forward propagation\n",
        "depends on the current values of model parameters $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$.\n",
        "They are given by the optimization algorithm according to backpropagation in the most recent iteration.\n",
        "On the other hand,\n",
        "the gradient calculation for the parameter\n",
        ":eqref:`eq_backprop-J-h` during backpropagation\n",
        "depends on the current value of the hidden layer output $\\mathbf{h}$,\n",
        "which is given by forward propagation.\n",
        "\n",
        "\n",
        "Therefore when training neural networks, once model parameters are initialized,\n",
        "we alternate forward propagation with backpropagation,\n",
        "updating model parameters using gradients given by backpropagation.\n",
        "Note that backpropagation reuses the stored intermediate values from forward propagation to avoid duplicate calculations.\n",
        "One of the consequences is that we need to retain\n",
        "the intermediate values until backpropagation is complete.\n",
        "This is also one of the reasons why training\n",
        "requires significantly more memory than plain prediction.\n",
        "Besides, the size of such intermediate values is roughly\n",
        "proportional to the number of network layers and the batch size.\n",
        "Thus,\n",
        "training deeper networks using larger batch sizes\n",
        "more easily leads to *out-of-memory* errors.\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer.\n",
        "Backpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order.\n",
        "When training deep learning models, forward propagation and backpropagation are interdependent,\n",
        "and training requires significantly more memory than prediction.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Assume that the inputs $\\mathbf{X}$ to some scalar function $f$ are $n \\times m$ matrices. What is the dimensionality of the gradient of $f$ with respect to $\\mathbf{X}$?\n",
        "1. Add a bias to the hidden layer of the model described in this section (you do not need to include bias in the regularization term).\n",
        "    1. Draw the corresponding computational graph.\n",
        "    1. Derive the forward and backward propagation equations.\n",
        "1. Compute the memory footprint for training and prediction in the model described in this section.\n",
        "1. Assume that you want to compute second derivatives. What happens to the computational graph? How long do you expect the calculation to take?\n",
        "1. Assume that the computational graph is too large for your GPU.\n",
        "    1. Can you partition it over more than one GPU?\n",
        "    1. What are the advantages and disadvantages over training on a smaller minibatch?\n",
        "\n",
        "[Discussions](https://discuss.d2l.ai/t/102)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propogation:\n",
        "\n",
        "Assume that the input x is of R^d, and hidden layer doesn't have a bias (just for simplicity)\n",
        "\n",
        "Then, our hidden layer weights W_1 is of R^h * d\n",
        "\n",
        "So, our intermediate variable is z = W_1x\n",
        "\n",
        "Then we apply our nonlinearity: h = ϕ(z)\n",
        "\n",
        "Then, we have a vector h of R^h\n",
        "\n",
        "We can then get our output, where the weights W_2 are of R^q * h\n",
        "\n",
        "So, o = W_2h\n",
        "\n",
        "Assume loss function = l, then we can calculate the loss for a single example: L = l(o, y)\n",
        "\n",
        "As for l2 reg (discussed in more detail later), we have:\n",
        "\n",
        "s = lambda/2 * (frobenius(W_1)^2 + frobenius(W_2)^2)\n",
        "\n",
        "So, our regularized loss, also called the objective function, is J = L + s\n",
        "\n",
        "Here is the computational graph of this network (squares denote variables and circles denote operators. The lower-left corner signifies the input and the upper-right corner is the output. Notice that the directions of the arrows, which illustrate data flow, are primarily rightward and upward):\n",
        "\n",
        "\n",
        "\n",
        "Backpropagation:\n",
        "\n",
        "Backprop is where we calculate the gradient of the neural network parameters. We do this in reverse, from the output to the input layer, using the chain rule.  Intermediate values (the partial derivatives) are stored during this process, assuming we are calculating the gradient with respect to the parameters.\n",
        "\n",
        "If we have functions Z = g(Y) and Y = f(X), where the input and output are tensors of arbitrary shape, We can calulate the derivative of Z with respect to X as:\n",
        "\n",
        "∂Z/∂X = prod(∂Z, ∂X)\n",
        "\n",
        "prod multiplies its arguments, and takes into account transposition and swapping input position and that sort of stuff. This is simple for vectors, it is just a matrix matrix multiplication. Higher dimensional tensors have their corresponding operations. prod is used to hide this overhead.\n",
        "\n",
        "We will use the same network as for forward pass above.\n",
        "\n",
        "So, we are trying to find the gradients ∂J/∂W_1 and ∂J/∂W_2\n",
        "\n",
        "We do this by starting at J and working our way back.\n",
        "\n",
        "First step: calculate the gradients of the objective function, J = L + s\n",
        "\n",
        "∂J/∂L = 1\n",
        "\n",
        "∂J/∂s = 1\n",
        "\n",
        "\n",
        "Seconds step: Calculate the gradients of the objective function with respect to the output layer o, using chain rule:\n",
        "\n",
        "∂L/∂o = prod(∂J/∂L, ∂L/∂o) = ∂L/∂o, R^q\n",
        "\n",
        "\n",
        "Third step: Caluclate the gradients of the regularization with wrt both parameters:\n",
        "\n",
        "s = lambda/2 * (frobenius(W_1)^2 + frobenius(W_2)^2)\n",
        "\n",
        "∂s/∂W_1 = lambda * W_1\n",
        "\n",
        "∂s/∂W_2 = lambda * W_2\n",
        "\n",
        "\n",
        "Fourth step: Calculate the gradients of the layer closest to the output, ∂J/∂W_2, R^q * h\n",
        "\n",
        "o = W_2h\n",
        "\n",
        "∂J/∂W_2 = prod(∂J/∂o, ∂o/∂W_2) + prod(∂J/∂s, ∂s/∂W_2)\n",
        "\n",
        "∂J/∂W_2 = (∂J/∂o)hT + lambda * W_2\n",
        "\n",
        "\n",
        "Fifth step: We then continue on this process, from the output layer to the hidden layer, calculate the gradient ∂J/∂h, R^h\n",
        "\n",
        "o = W_2h\n",
        "\n",
        "∂J/∂h = prod(∂J/∂o, ∂o/∂h) = W_2T(∂J/∂o)\n",
        "\n",
        "\n",
        "Sixth step: As the activation function ϕ applies elementwise, to find the gradient of intermediate variable z, ∂J/∂z, R^h, we use the elementwise operator ⊙. (Also because it applies elementwise, this is why neurons that are not activated do not recieved updates when using ReLU)\n",
        "\n",
        "h = ϕ(z)\n",
        "\n",
        "∂J/∂z = prod(∂J/∂h, ∂h/∂z) = (∂J/∂h)⊙ϕ'(z)\n",
        "\n",
        "\n",
        "Last step: Now, we can obtain ∂J/∂W_1, R^h * d, the gradients of the weights closest to the input layer\n",
        "\n",
        "z = W_1x\n",
        "\n",
        "∂J/∂W_1 = prod(∂J/∂z, ∂z/∂W_1) + prod(∂J/∂s, ∂s/∂W_1)\n",
        "\n",
        "∂J/∂W_1 = (∂J/∂z)xT + lambda * W_1\n",
        "\n",
        "\n",
        "\n",
        "Training Neural Networks:\n",
        "\n",
        "Clearly, forward and backward propagation depend on eachother. During forward propagation, we traverse the computational graph, and compute all variables on it. In backwards propagation, we use the values of the variables in reverse while finding the gradients.\n",
        "\n",
        "In the computational graph of our network above, this is clear. In our forward pass, we use the current values of W_1 and W_2 to find the regularization constant, and these weights themselves were given through the last backwards pass. And for calculating the gradients of W_2, we need the current value of h, which was given through the forward pass.\n",
        "\n",
        "As such, after our model parameters are initialized, we switch between forward and backward passes, updating the parameters using the gradients from backpropagation. We store the intermediate values of each forward pass, as we use them again for the corresponding backward pass so that we do not have to calculate it twice. This is why training requires more memory than inference, and the size of intermediate values is fairly proportional to the number of layers and batch size. Due to storing many parameters, this leads to a higher risk of out of memory errors.\n",
        "\n",
        "\n",
        "\n",
        "Exercises:\n",
        "\n",
        "1.Assume that the inputs  X to some scalar function  f  are  n * m  matrices. What is the dimensionality of the gradient of  f  with respect to  X?\n",
        "\n",
        "The dimensionality of the gradient wrt to the inputs is just the dimensionality of the inputs. As such, the gradients of f wrt X is n * m.\n",
        "\n",
        "2. Add a bias to the hidden layer of the model described in this section (you do not need to include bias in the regularization term).\n",
        "\n",
        "2.1 Draw the corresponding computational graph.\n",
        "\n",
        "\n",
        "2.2 Derive the forward and backward propagation equations.\n",
        "\n",
        "Forward:\n",
        "\n",
        "Assume that the input x is of R^d, and hidden layer bias b_1 of R^h\n",
        "\n",
        "Then, our hidden layer weights W_1 is of R^h * d\n",
        "\n",
        "So, our intermediate variable is z = W_1x + b_1\n",
        "\n",
        "Then we apply our nonlinearity: h = ϕ(z)\n",
        "\n",
        "Then, we have a vector h of R^h\n",
        "\n",
        "We can then get our output, where the weights W_2 are of R^q * h\n",
        "\n",
        "So, o = W_2h\n",
        "\n",
        "Assume loss function = l, then we can calculate the loss for a single example: L = l(o, y)\n",
        "\n",
        "As for l2 reg (discussed in more detail later), we have:\n",
        "\n",
        "s = lambda/2 * (frobenius(W_1)^2 + frobenius(W_2)^2)\n",
        "\n",
        "So, our regularized loss, also called the objective function, is J = L + s\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Backward:\n",
        "\n",
        "First step: calculate the gradients of the objective function, J = L + s\n",
        "\n",
        "∂J/∂L = 1\n",
        "\n",
        "∂J/∂s = 1\n",
        "\n",
        "\n",
        "Seconds step: Calculate the gradients of the objective function with respect to the output layer o, using chain rule:\n",
        "\n",
        "∂L/∂o = prod(∂J/∂L, ∂L/∂o) = ∂L/∂o, R^q\n",
        "\n",
        "\n",
        "Third step: Caluclate the gradients of the regularization with wrt both parameters:\n",
        "\n",
        "s = lambda/2 * (frobenius(W_1)^2 + frobenius(W_2)^2)\n",
        "\n",
        "∂s/∂W_1 = lambda * W_1\n",
        "\n",
        "∂s/∂W_2 = lambda * W_2\n",
        "\n",
        "\n",
        "Fourth step: Calculate the gradients of the layer closest to the output, ∂J/∂W_2, R^q * h\n",
        "\n",
        "o = W_2h\n",
        "\n",
        "∂J/∂W_2 = prod(∂J/∂o, ∂o/∂W_2) + prod(∂J/∂s, ∂s/∂W_2)\n",
        "\n",
        "∂J/∂W_2 = (∂J/∂o)hT + lambda * W_2\n",
        "\n",
        "\n",
        "Fifth step: We then continue on this process, from the output layer to the hidden layer, calculate the gradient ∂J/∂h, R^h\n",
        "\n",
        "o = W_2h\n",
        "\n",
        "∂J/∂h = prod(∂J/∂o, ∂o/∂h) = W_2T(∂J/∂o)\n",
        "\n",
        "\n",
        "Sixth step: As the activation function ϕ applies elementwise, to find the gradient of intermediate variable z, ∂J/∂z, R^h, we use the elementwise operator ⊙. (Also because it applies elementwise, this is why neurons that are not activated do not recieved updates when using ReLU)\n",
        "\n",
        "h = ϕ(z)\n",
        "\n",
        "∂J/∂z = prod(∂J/∂h, ∂h/∂z) = (∂J/∂h)⊙ϕ'(z)\n",
        "\n",
        "\n",
        "Sixth step: Now, we can obtain ∂J/∂W_1, R^h * d, the gradients of the weights closest to the input layer\n",
        "\n",
        "z = W_1x + b_1\n",
        "\n",
        "∂J/∂W_1 = prod(∂J/∂z, ∂z/∂W_1) + prod(∂J/∂s, ∂s/∂W_1)\n",
        "\n",
        "∂J/∂W_1 = (∂J/∂z)xT + lambda * W_1\n",
        "\n",
        "\n",
        "Seventh step: Now, we can obtain ∂J/∂W_1, R^h, the gradient of the bias\n",
        "\n",
        "z = W_1x + b_1\n",
        "\n",
        "∂J/∂b_1 = prod(∂J/∂z, ∂z/∂b_1)\n",
        "\n",
        "∂J/∂b_1 = (∂J/∂z)\n",
        "\n",
        "\n",
        "The derivative of the weights doesn't change, because these are affine functions and bias doesn't affect slope.\n",
        "\n",
        "\n",
        "\n",
        "3. Compute the memory footprint for training and prediction in the model described in this section.\n",
        "\n",
        "we used a 3 layer MLP, with dimensions R ^:\n",
        "\n",
        "input = d\n",
        "\n",
        "W_1 = d * h\n",
        "\n",
        "b_1 = h\n",
        "\n",
        "z = h\n",
        "\n",
        "W_2 = h * q\n",
        "\n",
        "output = q\n",
        "\n",
        "During training, for the forward pass we store all of these, so we store (data type size) * (d + d * h + h + h + h * q + q) = (data type) * (d + d * h + 2 * h + h * q + q) bits as a rough estimate. And then for our backward pass, we use these to calculate the gradients. For example, during backpropagation, when we compute ∂J/∂W_2 = (∂J/∂o)hT + lambda * W_2, we need the value of h that was computed during the forward pass. If we had discarded h after computing o, we'd have to recompute it, which defeats the purpose of backpropagation's efficiency. Which means we need to store (d * h + h + h * q) more, and update W_1, W_2, and b_1. Then, we can clear everything other than those 3. This cycle then repeats.\n",
        "\n",
        "During prediction, since we are no longer updating the model, we do not need to store these after they are used. As we progress from input to output, once we finish the step where the values are used in calculations we can clear them. Like after you are finished W_1x, you can clear x, which is input of R^d.\n",
        "\n",
        "So, the difference is that you have to retain the intermediate values during training, but not during inference. This means that training requires vastly more memory, and is far more likely to be subject to out of memory errors, especially as models get deeper, since amount of intermediate values stored is roughly proportional to batch_size * depth * (neurons_per_layer + parameters_per_layer).\n",
        "\n",
        "\n",
        "4. Assume that you want to compute second derivatives. What happens to the computational graph? How long do you expect the calculation to take?\n",
        "\n",
        "The second derivative, or hessian, means you take the derivative of the gradient. This means that if your gradient is of R ^ n, the hessian will be of R ^ n * n. This doesn't affect the computational graph for the forward pass, as the hessian is based off of the gradient, which we already have when doing a backward pass. However, during the backward pass, we need to store the size of the square of each weight and bias as well now.\n",
        "\n",
        "Additionally, the result of this is that our graph is no longer identical forward and backward, as after computing the gradient, you treat each component f_i(x) as a scalar function, and compute the derivative of all f_i wrt to that f_i. As such, you need n extra backward passes (using the naive approach), to get the hessian.\n",
        "\n",
        "This should go from O(n) to O(n^2), which is obviously terrible, which is why in practice the full hessian is not calculated, and methods such as diagonal approximations (only diagonals of hessian), or low rank approximations are used.\n",
        "\n",
        "\n",
        "\n",
        "5. Assume that the computational graph is too large for your GPU.\n",
        "\n",
        "5.1 Can you partition it over more than one GPU?\n",
        "\n",
        "Yes, you can.\n",
        "\n",
        "There are 2 main strategies.\n",
        "\n",
        "Pipeline parallelism:\n",
        "If you take any variable on the computational graph, the backward pass path to whatever foundational inputs lead to that variable are all the variables you need to store. However, each variable after that only needs access to the last \"step\" of variables.\n",
        "\n",
        "So in our example, where we have x -> z -> h -> o, if you store x, z, and h on GPU 1, and o on GPU 2, then for GPU 2 to calculate o = W_2h, it only needs h from GPU 1 and doesn't need to know x and z.\n",
        "\n",
        "This reduces intermediate storage per gpu, but also means that there are dependencies, as each GPU must wait on the previous one.\n",
        "\n",
        "Data/Model parallelism:\n",
        "We can split inputs and weights and biases and all that across gpus, but keeping the full computational graph. Because dot products are elementwise independent, each gpu can store a subset of the tensors, and then each return a scalar which is summed to make the final scalar. Each GPU handles part of the computation and results get combined. This means that there is no waiting for other GPUs (well at least not for sequential resources in the same way), as they operate breadthwise parallel.\n",
        "\n",
        "\n",
        "5.2\n",
        "\n",
        "The advantages of a smaller minibatches is that intermediate values stores grow roughly proportional to batch_size * depth * (neurons_per_layer + parameters_per_layer). As such, a smaller batch size gives a linearly proportional reduction in the memory needed. This is obviously good, because it reduces the risk of out of memory errors.\n",
        "\n",
        "The disadvantages is that minibatches have higher variance and are less representative of the underlying distribution, which is inherent to their smaller size. As such, if you use anything like batchnorm, it will be less accurate, which can affect how well it converges.\n",
        "\n",
        "Additonally, GPU are designed for extreme parallelization. Using smaller minibatches just means you are not taking full advantage of that, as it means:\n",
        "Lower parallelization (underutilized GPU cores)\n",
        "Higher overhead per operation\n",
        "Worse arithmetic intensity (ratio of compute to memory operations)\n",
        "\n",
        "So in general, you want to maximize the minibatch size as much as your memory allows for.\n"
      ],
      "metadata": {
        "id": "3FzvvkTEK_0w"
      },
      "id": "3FzvvkTEK_0w"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}